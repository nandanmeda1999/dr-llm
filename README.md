

<div style="margin-top:50px; margin-left: 12%;">
  <h1 style="font-size: 30px; margin: 0;"> Dr.LLM: Dynamic Layer Routing in LLMs</h1>
</div>


<div align="left" style="margin:24px 0;">
  <img src="https://user-images.githubusercontent.com/74038190/212284115-f47cd8ff-2ffb-4b04-b5bf-4d1c14c0247f.gif"
       width="100%" height="4"/>
</div>

<p align="center">
  <a href="https://arxiv.org/pdf/2510.12773"><img src="https://img.shields.io/badge/arXiv-2510.12773-b31b1b.svg" alt="arXiv"></a>
</p>

<p align="center">
  <a href="https://www.linkedin.com/in/ahmed-heakl/"><b>Ahmed Heakl</b></a>, 
  <a href="https://scholar.google.com/citations?user=Jt4OYwMAAAAJ&hl=fr"><b>Martin Gubri</b></a>, 
  <a href="https://scholar.google.com/citations?user=M59O9lkAAAAJ&hl=en"><b>Salman Khan</b></a>, 
  <a href="https://scholar.google.com/citations?user=o0qtjzYAAAAJ&hl=en"><b>Sangdoo Yun</b></a>,
   <a href="https://seongjoonoh.com/"><b>Seong Joon Oh</b></a>,
</p>


<p align="center">
  <b>Parameter Lab</b> ¬∑ <b>MBZUAI</b> ¬∑ <b>NAVER AI Lab</b> ¬∑ <b>University of T√ºbingen</b> ¬∑ <b>T√ºbingen AI Center</b>
</p>

---

## üÜï Latest Updates
- üì¢ **15 October 2025**: Paper ArXived!


## Overview

<table>
<tr>
<td width="30%">
<img src="image.png" width="100%">
</td>
<td width="70%">
Large Language Models (LLMs) process every token through all layers of a transformer stack, causing wasted computation on simple queries and insufficient flexibility for harder ones that need deeper reasoning. Adaptive-depth methods can improve efficiency, but prior approaches rely on costly inference-time search, architectural changes, or large-scale retraining, and in practice often degrade accuracy
despite efficiency gains. We introduce Dr.LLM, Dynamic routing of Layers for LLMs, a retrofittable framework that equips pretrained models with lightweight per-layer routers deciding to skip, execute, or repeat a block. Routers are trained with explicit supervision: using Monte Carlo Tree Search (MCTS), we derive high-quality layer configurations that preserve or improve accuracy under a compute budget. Our design, windowed pooling for stable routing, focal loss with class balancing, and bottleneck MLP routers, ensures robustness under class imbalance and long sequences. 

On ARC (logic) and DART (math), Dr.LLM improves accuracy by up to +3.4%p while saving 5 layers per example on average.
Routers generalize to out-of-domain tasks (MMLU, GSM8k, AIME, TruthfulQA, SQuADv2, GPQA, PIQA, AGIEval) with only 0.85% accuracy drop while retaining efficiency, and outperform prior routing methods by up to +7.7%p. Overall, Dr.LLM shows that explicitly supervised routers retrofit frozen LLMs for budgetaware, accuracy-driven inference without altering base weights.
</td>
</tr>
</table>


## üß™ Evaluation

We evaluate **Dr.LLM** using [`lm-eval-harness`](https://github.com/EleutherAI/lm-evaluation-harness) across both **in-domain reasoning tasks** and **out-of-domain (OOD)** benchmarks.

### In-Domain (Training & Evaluation Tasks)

| Dataset                | Domain          | Metric   | Purpose                              |
| ---------------------- | --------------- | -------- | ------------------------------------ |
| **ARC-Easy/Challenge** | Logic reasoning | Accuracy | Test structured reasoning depth      |
| **DART (levels 1‚Äì5)**  | Math reasoning  | Accuracy | Test iterative, multi-step reasoning |

Dr.LLM routers are trained on 4K MCTS-derived execution paths from these datasets.


During inference, layer routing decisions are applied *per input sequence*, adding negligible overhead and remaining KV-cache compatible.

---

### Out-of-Domain (Generalization Benchmarks)

We evaluate zero-shot transfer on:

> **MMLU**, **GSM8k**, **AIME24**, **TruthfulQA**, **GPQA Diamond**, **SQuADv2**, **PIQA**, and **AGIEval**.

Dr.LLM achieves **only ‚àí0.85%p average accuracy drop** while maintaining efficiency across these unseen datasets ‚Äî showing strong generalization.

---

## üìä Results Summary

| Model             | Domain   | Œî Accuracy | Layers Saved |
| ----------------- | -------- | ---------- | ------------ |
| LLaMA-3B-Instruct | ARC+DART | **+2.7%p** | ‚àí7.4         |
| LLaMA-8B-Instruct | ARC+DART | **+2.3%p** | ‚àí8.7         |
| LLaMA-3B-Base     | ARC+DART | **+3.2%p** | ‚àí3.0         |
| LLaMA-8B-Base     | ARC+DART | **+2.4%p** | ‚àí4.2         |
| Qwen-3B-Instruct  | ARC+DART | **+2.3%p** | ‚àí3.3         |
| Qwen-7B-Instruct  | ARC+DART | **+0.9%p** | ‚àí3.4         |

> üß† Routers improve reasoning-heavy tasks by **up to +4.0%p accuracy** while skipping **5 layers per example** on average.

Compared to prior adaptive-depth methods (e.g., LayerSkip, FlexiDepth, MindSkip), **Dr.LLM**:

* Trains on only **4K MCTS paths** (vs 300K+ examples),
* Requires **no finetuning or base weight modification**,
* Outperforms SoTA methods by up to **+7.7%p accuracy**.



---

## ‚öôÔ∏è Usage

### 1Ô∏è‚É£ Installation

```bash
git clone https://github.com/parameterlab/dr-llm
cd dr-llm
pip install -r requirements.txt
```

<details>
<summary><b>2Ô∏è‚É£ Training the Routers </b></summary>

<!-- add a warning that the full code is not released yet -->
[!‚ö†Ô∏è Note: Full code release is pending. Contact <a href="https://www.linkedin.com/in/ahmed-heakl/">Ahmed Heakl</a> for access. ‚ö†Ô∏è!]

Training uses **AdamW**, 25 epochs, **1√ó10‚Åª¬≥ LR**, **bf16 precision**, and a **single A100 GPU (40GB)** ‚Äî taking under 4 hours.


Models source code must be manipulated to insert routers after each transformer block. 

Routers are trained separately using MCTS-generated supervision:

```bash
python train.py \
  --model llama-3-8b-instruct \
  --data_dir data/arc_dart \
  --save_dir checkpoints/drllm_router
```
</details>


### 3Ô∏è‚É£ Evaluation with lm-eval-harness

```bash
lm_eval \
  --model openai/llama-3-8b-instruct \
  --tasks arc_challenge,dart,gsm8k,mmlu \
  --device cuda
```


---


## üß≠ Citation

If you find this work useful, please cite:

```bibtex
@article{heakl2025drllm,
  title={Dr.LLM: Dynamic Layer Routing in LLMs},
  author={Ahmed Heakl and Martin Gubri and Salman Khan and Sangdoo Yun and Seong Joon Oh},
  journal={arXiv preprint arXiv:2510.12773},
  year={2025}
}
```

