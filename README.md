<h1 align="center">ğŸ§© Dr.LLM: Dynamic Layer Routing in LLMs</h1>
<p align="center">
  <img src="https://user-images.githubusercontent.com/74038190/212284115-f47cd8ff-2ffb-4b04-b5bf-4d1c14c0247f.gif" width="90%" height="4">
</p>

<p align="center">
<b>Ahmed Heakl</b> Â· <b>Martin Gubri</b> Â· <b>Salman Khan</b> Â· <b>Sangdoo Yun</b> Â· <b>Seong Joon Oh</b>  
<br>
<b>Parameter Lab</b> Â· <b>MBZUAI</b> Â· <b>NAVER AI Lab</b> Â· <b>University of TÃ¼bingen</b> Â· <b>TÃ¼bingen AI Center</b>
</p>

<p align="center">
  <a href="https://arxiv.org/pdf/2510.12773">
    <img src="https://img.shields.io/badge/arXiv-2510.12773-b31b1b.svg" alt="arXiv">
  </a>
</p>

---

<div align="center" style="background-color:#f9f9f9; padding:10px; border-radius:5px;">

### ğŸš¨ **IMPORTANT NOTICE â€” CODE RELEASE STATUS**

ğŸ§© The **training**, **data generation**, and **in-domain evaluation** code for **Dr.LLM** are **not yet released**.  
These components (**MCTS supervision**, **router training scripts**, and **lm-eval integration**) will be made public in an upcoming update.  
**Stay tuned for the full release!**

</div>






## ğŸ†• Latest Updates
- ğŸ“¢ **15 October 2025** â€” Paper ArXived!

## ğŸ“˜ Table of Contents
- [Overview](#overview)
- [ğŸ§ª Evaluation](#-evaluation)
  - [In-Domain (Training & Evaluation Tasks)](#in-domain-training--evaluation-tasks)
  - [Out-of-Domain (Generalization Benchmarks)](#out-of-domain-generalization-benchmarks)
- [ğŸ“Š Results Summary](#-results-summary)
- [âš™ï¸ Usage](#ï¸-usage)
  - [Installation](#1ï¸âƒ£-installation)
  - [Training the Routers](#2ï¸âƒ£-training-the-routers)
  - [Evaluation with lm-eval-harness](#3ï¸âƒ£-evaluation-with-lm-eval-harness)
- [ğŸ§­ Citation](#-citation)


## ğŸ§© Overview

<p align="center">
  <img src="assets/teaser.png" width="50%" alt="Dr.LLM Teaser">
</p>

Large Language Models (LLMs) process every token through all layers of a transformer stack, wasting compute on simple queries and lacking flexibility for harder ones that need deeper reasoning.  

**Dr.LLM (Dynamic Routing of Layers for LLMs)** is a retrofittable framework that adds lightweight per-layer routers to pretrained models.  
Each router decides whether to skip, execute, or repeat a layer, enabling adaptive depth without retraining or architectural changes.

Routers are trained with explicit supervision from Monte Carlo Tree Search (MCTS), generating high-quality layer configurations that preserve or improve accuracy under a compute budget.  
Stabilized with windowed pooling, focal loss, and bottleneck MLPs, Dr.LLM maintains robustness under class imbalance and long sequences.

ğŸ“ˆ **Results**
- On ARC (logic) and DART (math), Dr.LLM improves accuracy by **+3.4%p** while saving **~5 layers** per input.
- Routers generalize to MMLU, GSM8k, AIME, TruthfulQA, SQuADv2, GPQA, PIQA, and AGIEval with only **0.85% accuracy drop**.
- Outperforms prior routing methods (LayerSkip, FlexiDepth, MindSkip) by up to **+7.7%p**.

> ğŸ’¡ Dr.LLM equips frozen LLMs for **budget-aware**, **accuracy-driven inference** â€” no base weight modification required.


## ğŸ§ª Evaluation

We evaluate **Dr.LLM** using [`lm-eval-harness`](https://github.com/EleutherAI/lm-evaluation-harness) across both **in-domain reasoning tasks** and **out-of-domain (OOD)** benchmarks.

### In-Domain (Training & Evaluation Tasks)

| Dataset                | Domain          | Metric   | Purpose                              |
| ---------------------- | --------------- | -------- | ------------------------------------ |
| **ARC-Easy/Challenge** | Logic reasoning | Accuracy | Test structured reasoning depth      |
| **DART (levels 1â€“5)**  | Math reasoning  | Accuracy | Test iterative, multi-step reasoning |

Dr.LLM routers are trained on 4K MCTS-derived execution paths from these datasets.


During inference, layer routing decisions are applied *per input sequence*, adding negligible overhead and remaining KV-cache compatible.

---

### Out-of-Domain (Generalization Benchmarks)

We evaluate zero-shot transfer on:

> **MMLU**, **GSM8k**, **AIME24**, **TruthfulQA**, **GPQA Diamond**, **SQuADv2**, **PIQA**, and **AGIEval**.

Dr.LLM achieves **only âˆ’0.85%p average accuracy drop** while maintaining efficiency across these unseen datasets â€” showing strong generalization.

---

## ğŸ“Š Results Summary

| Model             | Domain   | Î” Accuracy | Layers Saved |
| ----------------- | -------- | ---------- | ------------ |
| LLaMA-3B-Instruct | ARC+DART | **+2.7%p** | âˆ’7.4         |
| LLaMA-8B-Instruct | ARC+DART | **+2.3%p** | âˆ’8.7         |
| LLaMA-3B-Base     | ARC+DART | **+3.2%p** | âˆ’3.0         |
| LLaMA-8B-Base     | ARC+DART | **+2.4%p** | âˆ’4.2         |
| Qwen-3B-Instruct  | ARC+DART | **+2.3%p** | âˆ’3.3         |
| Qwen-7B-Instruct  | ARC+DART | **+0.9%p** | âˆ’3.4         |

> ğŸ§  Routers improve reasoning-heavy tasks by **up to +4.0%p accuracy** while skipping **5 layers per example** on average.

Compared to prior adaptive-depth methods (e.g., LayerSkip, FlexiDepth, MindSkip), **Dr.LLM**:

* Trains on only **4K MCTS paths** (vs 300K+ examples),
* Requires **no finetuning or base weight modification**,
* Outperforms SoTA methods by up to **+7.7%p accuracy**.

---

## âš™ï¸ Usage

### 1ï¸âƒ£ Installation

```bash
git clone https://github.com/parameterlab/dr-llm
cd dr-llm
pip install -r requirements.txt
```

<details>
<summary><b>2ï¸âƒ£ Training the Routers </b></summary>

> âš ï¸ Note: Full code release is pending âš ï¸

Training uses **AdamW**, 25 epochs, **1Ã—10â»Â³ LR**, **bf16 precision**, and a **single A100 GPU (40GB)** â€” taking under 4 hours.


Models source code must be manipulated to insert routers after each transformer block. 

Routers are trained separately using MCTS-generated supervision:

```bash
python train.py \
  --model llama-3-8b-instruct \
  --data_dir data/arc_dart \
  --save_dir checkpoints/drllm_router
```
</details>

<details>
<summary><b>3ï¸âƒ£ Evaluation with lm-eval-harness</b></summary>

> ğŸš¨ Note: Full code release is pending ğŸš¨

```bash
lm_eval \
  --model openai/llama-3-8b-instruct \
  --tasks arc_challenge,dart,gsm8k,mmlu \
  --device cuda
```
</details>


---


## ğŸ§­ Citation

If you find this work useful, please cite:

```bibtex
@article{heakl2025drllm,
  title={Dr.LLM: Dynamic Layer Routing in LLMs},
  author={Ahmed Heakl and Martin Gubri and Salman Khan and Sangdoo Yun and Seong Joon Oh},
  journal={arXiv preprint arXiv:2510.12773},
  year={2025}
}
```

